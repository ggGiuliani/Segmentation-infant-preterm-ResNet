{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Prova Resnet.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/ggGiuliani/Segmentation-infant-preterm-ResNet/blob/master/Prova_Resnet.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "V8hrG5pSaV-_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#RESNET\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "MF_4suaEaRFK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "autenticazione Google Drive"
      ]
    },
    {
      "metadata": {
        "id": "kNMCN5RWaPpB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Install the PyDrive wrapper & import libraries.\n",
        "# This only needs to be done once in a notebook.\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once in a notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7ZXMVHvxcoT5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Lista file .zip con relativo id"
      ]
    },
    {
      "metadata": {
        "id": "tWu3T4dGcsiz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "eb7c8e71-59e8-4581-81bf-5cf190274805"
      },
      "cell_type": "code",
      "source": [
        "# List .txt files in the root.\n",
        "#\n",
        "# Search query reference:\n",
        "# https://developers.google.com/drive/v2/web/search-parameters\n",
        "#listed = drive.ListFile({'q': \"title contains '.txt' and 'root' in parents\"}).GetList()\n",
        "listed = drive.ListFile({'q': \"title contains '.zip' and 'root' in parents\"}).GetList()\n",
        "for file in listed:\n",
        "  print('title {}, id {}'.format(file['title'], file['id']))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "title resnet.zip, id 1jPPiL8D-xYHC9_luIxn3H-C5wP4sgBq0\n",
            "title apk app.zip, id 1owRoqzrfsIZFJ1X6tVxrA9Gsiqo1KVym\n",
            "title Unify-v1.6.zip, id 13amwzUj0C-Th5t0GjjW8k-ubG6xHD2CQ\n",
            "title unet.zip, id 1rtfjIf71naDFm-3eRyhqn2Z5zf3X3JTS\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CXJsTTjGcx9h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Upload File tramite id (modificare id file ricavato da sopra)"
      ]
    },
    {
      "metadata": {
        "id": "ENEdCsKNc7qR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "808011a4-ec0f-41a1-8c2a-272f8946b115"
      },
      "cell_type": "code",
      "source": [
        "fileId = drive.CreateFile({'id': '1jPPiL8D-xYHC9_luIxn3H-C5wP4sgBq0'}) #DRIVE_FILE_ID is file id example: 1iytA1n2z4go3uVCwE_vIKouTKyIDjEq\n",
        "fileId.GetContentFile(fileId['title'])  # Save Drive file as a local file\n",
        "print('Uploaded {}, id {}'.format(fileId['title'], fileId['id']))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uploaded resnet.zip, id 1jPPiL8D-xYHC9_luIxn3H-C5wP4sgBq0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UkDQ9GwklLQi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "estraggo resnet.zip"
      ]
    },
    {
      "metadata": {
        "id": "pzbyM8StlN0U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!unzip resnet.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "58PSmefblT9m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Test GPU\n",
        "\n",
        "Go to Runtime\n",
        "\n",
        "select accelerator: GPU\n",
        "\n",
        "SAVE"
      ]
    },
    {
      "metadata": {
        "id": "cp3gTNkqlaFs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dc46325b-20c2-447d-96f0-b163f3005cdc"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WLzPIByRle0x",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Traning and Test RESNET"
      ]
    },
    {
      "metadata": {
        "id": "DGj-B5xpljDG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        },
        "outputId": "3ba4b4bd-1a53-48dc-a14b-ad0052f78d52"
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "from keras import backend as K\n",
        "from keras import layers\n",
        "from keras.callbacks import ModelCheckpoint, CSVLogger\n",
        "from keras.layers import Activation\n",
        "from keras.layers import Input, Conv2D, ZeroPadding2D, MaxPooling2D, UpSampling2D, concatenate\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "from skimage.io import imsave\n",
        "\n",
        "from data import load_train_data, load_test_data\n",
        "\n",
        "K.set_image_data_format('channels_last')  # TF dimension ordering in this code\n",
        "\n",
        "img_rows = 96\n",
        "img_cols = 128\n",
        "\n",
        "\n",
        "\n",
        "smooth = 1.\n",
        "epochs = 20\n",
        "\n",
        "\n",
        "def dice_coef(y_true, y_pred):\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
        "\n",
        "\n",
        "def dice_coef_loss(y_true, y_pred):\n",
        "    return -dice_coef(y_true, y_pred)\n",
        "\n",
        "\n",
        "def precision(y_true, y_pred):\n",
        "    \"\"\"Precision metric.\n",
        "    Only computes a batch-wise average of precision.\n",
        "    Computes the precision, a metric for multi-label classification of\n",
        "    how many selected items are relevant.\n",
        "    \"\"\"\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "\n",
        "def recall(y_true, y_pred):\n",
        "    \"\"\"Recall metric.\n",
        "    Only computes a batch-wise average of recall.\n",
        "    Computes the recall, a metric for multi-label classification of\n",
        "    how many relevant items are selected.\n",
        "    \"\"\"\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "\n",
        "def f1score(y_true, y_pred):\n",
        "    def recall(y_true, y_pred):\n",
        "        \"\"\"Recall metric.\n",
        "        Only computes a batch-wise average of recall.\n",
        "        Computes the recall, a metric for multi-label classification of\n",
        "        how many relevant items are selected.\n",
        "        \"\"\"\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "        recall = true_positives / (possible_positives + K.epsilon())\n",
        "        return recall\n",
        "\n",
        "    def precision(y_true, y_pred):\n",
        "        \"\"\"Precision metric.\n",
        "        Only computes a batch-wise average of precision.\n",
        "        Computes the precision, a metric for multi-label classification of\n",
        "        how many selected items are relevant.\n",
        "        \"\"\"\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "        precision = true_positives / (predicted_positives + K.epsilon())\n",
        "        return precision\n",
        "\n",
        "    precision = precision(y_true, y_pred)\n",
        "    recall = recall(y_true, y_pred)\n",
        "    return 2 * ((precision * recall) / (precision + recall))\n",
        "\n",
        "\n",
        "def identity_block(input_tensor, kernel_size, filters, stage, block):\n",
        "    filters1, filters2, filters3 = filters\n",
        "    if K.image_data_format() == 'channels_last':\n",
        "        bn_axis = 3\n",
        "    else:\n",
        "        bn_axis = 1\n",
        "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
        "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
        "\n",
        "    x = Conv2D(filters1, (1, 1), name=conv_name_base + '2a')(input_tensor)\n",
        "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2a')(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = Conv2D(filters2, kernel_size, padding='same', name=conv_name_base + '2b')(x)\n",
        "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2b')(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = Conv2D(filters3, (1, 1), name=conv_name_base + '2c')(x)\n",
        "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2c')(x)\n",
        "\n",
        "    x = layers.add([x, input_tensor])\n",
        "    x = Activation('relu')(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def conv_block(input_tensor, kernel_size, filters, stage, block, strides=(2, 2)):\n",
        "    filters1, filters2, filters3 = filters\n",
        "    if K.image_data_format() == 'channels_last':\n",
        "        bn_axis = 3\n",
        "    else:\n",
        "        bn_axis = 1\n",
        "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
        "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
        "\n",
        "    x = Conv2D(filters1, (1, 1), strides=strides, name=conv_name_base + '2a')(input_tensor)\n",
        "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2a')(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = Conv2D(filters2, kernel_size, padding='same', name=conv_name_base + '2b')(x)\n",
        "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2b')(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = Conv2D(filters3, (1, 1), name=conv_name_base + '2c')(x)\n",
        "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2c')(x)\n",
        "\n",
        "    shortcut = Conv2D(filters3, (1, 1), strides=strides, name=conv_name_base + '1')(input_tensor)\n",
        "    shortcut = BatchNormalization(axis=bn_axis, name=bn_name_base + '1')(shortcut)\n",
        "\n",
        "    x = layers.add([x, shortcut])\n",
        "    x = Activation('relu')(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def up_conv_block(input_tensor, kernel_size, filters, stage, block, strides=(1, 1)):\n",
        "    filters1, filters2, filters3 = filters\n",
        "    if K.image_data_format() == 'channels_last':\n",
        "        bn_axis = 3\n",
        "    else:\n",
        "        bn_axis = 1\n",
        "    up_conv_name_base = 'up' + str(stage) + block + '_branch'\n",
        "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
        "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
        "\n",
        "    x = UpSampling2D(size=(2, 2), name=up_conv_name_base + '2a')(input_tensor)\n",
        "\n",
        "    x = Conv2D(filters1, (1, 1), strides=strides, name=conv_name_base + '2a')(x)\n",
        "\n",
        "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2a')(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = Conv2D(filters2, kernel_size, padding='same', name=conv_name_base + '2b')(x)\n",
        "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2b')(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = Conv2D(filters3, (1, 1), name=conv_name_base + '2c')(x)\n",
        "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2c')(x)\n",
        "\n",
        "    shortcut = UpSampling2D(size=(2, 2), name=up_conv_name_base + '1')(input_tensor)\n",
        "    shortcut = Conv2D(filters3, (1, 1), strides=strides, name=conv_name_base + '1')(shortcut)\n",
        "    shortcut = BatchNormalization(axis=bn_axis, name=bn_name_base + '1')(shortcut)\n",
        "\n",
        "    x = layers.add([x, shortcut])\n",
        "    x = Activation('relu')(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def get_resnet(f=16, bn_axis=3, classes=1):\n",
        "    input = Input((img_rows, img_cols, 1))\n",
        "    x = ZeroPadding2D((4, 4))(input)\n",
        "    x = Conv2D(f, (7, 7), strides=(2, 2), name='conv1')(x)\n",
        "    x = BatchNormalization(axis=bn_axis, name='bn_conv1')(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
        "\n",
        "    x = conv_block(x, 3, [f, f, f * 2], stage=2, block='a', strides=(1, 1))\n",
        "    x = identity_block(x, 3, [f, f, f * 2], stage=2, block='b')\n",
        "    x2 = identity_block(x, 3, [f, f, f * 2], stage=2, block='c')\n",
        "\n",
        "    x = conv_block(x2, 3, [f * 2, f * 2, f * 4], stage=3, block='a')\n",
        "    x = identity_block(x, 3, [f * 2, f * 2, f * 4], stage=3, block='b')\n",
        "    x3 = identity_block(x, 3, [f * 2, f * 2, f * 4], stage=3, block='d')\n",
        "\n",
        "    x = conv_block(x3, 3, [f * 4, f * 4, f * 8], stage=4, block='a')\n",
        "    x = identity_block(x, 3, [f * 4, f * 4, f * 8], stage=4, block='b')\n",
        "    x4 = identity_block(x, 3, [f * 4, f * 4, f * 8], stage=4, block='f')\n",
        "\n",
        "    x = conv_block(x4, 3, [f * 8, f * 8, f * 16], stage=5, block='a')\n",
        "    x = identity_block(x, 3, [f * 8, f * 8, f * 16], stage=5, block='b')\n",
        "    x = identity_block(x, 3, [f * 8, f * 8, f * 16], stage=5, block='c')\n",
        "\n",
        "    x = up_conv_block(x, 3, [f * 16, f * 8, f * 8], stage=6, block='a')\n",
        "    x = identity_block(x, 3, [f * 16, f * 8, f * 8], stage=6, block='b')\n",
        "    x = identity_block(x, 3, [f * 16, f * 8, f * 8], stage=6, block='c')\n",
        "\n",
        "    x = concatenate([x, x4], axis=bn_axis)\n",
        "\n",
        "    x = up_conv_block(x, 3, [f * 16, f * 4, f * 4], stage=7, block='a')\n",
        "    x = identity_block(x, 3, [f * 16, f * 4, f * 4], stage=7, block='b')\n",
        "\n",
        "    x = identity_block(x, 3, [f * 16, f * 4, f * 4], stage=7, block='f')\n",
        "\n",
        "    x = concatenate([x, x3], axis=bn_axis)\n",
        "\n",
        "    x = up_conv_block(x, 3, [f * 8, f * 2, f * 2], stage=8, block='a')\n",
        "    x = identity_block(x, 3, [f * 8, f * 2, f * 2], stage=8, block='b')\n",
        "    x = identity_block(x, 3, [f * 8, f * 2, f * 2], stage=8, block='d')\n",
        "\n",
        "    x = concatenate([x, x2], axis=bn_axis)\n",
        "\n",
        "    x = up_conv_block(x, 3, [f * 4, f, f], stage=10, block='a', strides=(1, 1))\n",
        "    x = identity_block(x, 3, [f * 4, f, f], stage=10, block='b')\n",
        "    x = identity_block(x, 3, [f * 4, f, f], stage=10, block='c')\n",
        "\n",
        "    x = UpSampling2D(size=(2, 2))(x)\n",
        "    x = Conv2D(classes, (3, 3), padding='same', activation='sigmoid', name='convLast')(x)\n",
        "\n",
        "    model = Model(input, x, name='resnetUnet')\n",
        "    model.compile(optimizer=Adam(lr=3e-4), loss=dice_coef_loss,\n",
        "                  metrics=[dice_coef, 'accuracy', precision, recall, f1score])\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def train_and_predict(bit):\n",
        "    print('-' * 30)\n",
        "    print('Loading and train data (bit = ' + str(bit) + ') ...')\n",
        "    print('-' * 30)\n",
        "    imgs_bit_train, imgs_bit_mask_train, _ = load_train_data(bit)\n",
        "\n",
        "    print(imgs_bit_train.shape[0], imgs_bit_mask_train.shape[0])\n",
        "\n",
        "    imgs_bit_train = imgs_bit_train.astype('float32')\n",
        "    mean = np.mean(imgs_bit_train)\n",
        "    std = np.std(imgs_bit_train)\n",
        "\n",
        "    imgs_bit_train -= mean\n",
        "    imgs_bit_train /= std\n",
        "\n",
        "    imgs_bit_mask_train = imgs_bit_mask_train.astype('float32')\n",
        "    imgs_bit_mask_train /= 255.  # scale masks to [0, 1]\n",
        "\n",
        "    print('-' * 30)\n",
        "    print('Creating and compiling model (bit = ' + str(bit) + ') ...')\n",
        "    print('-' * 30)\n",
        "    model = get_resnet(f=16, bn_axis=3, classes=1)\n",
        "\n",
        "    csv_logger = CSVLogger('log_resnet_' + str(bit) + '.csv')\n",
        "    model_checkpoint = ModelCheckpoint('weights_resnet_' + str(bit) + '.h5', monitor='val_loss', save_best_only=True)\n",
        "\n",
        "    print('-' * 30)\n",
        "    print('Fitting model (bit = ' + str(bit) + ') ...')\n",
        "    print('-' * 30)\n",
        "\n",
        "    model.fit(imgs_bit_train, imgs_bit_mask_train, batch_size=32, epochs=epochs, verbose=1, shuffle=True,\n",
        "              validation_split=0.2,\n",
        "              callbacks=[csv_logger, model_checkpoint])\n",
        "\n",
        "    print('-' * 30)\n",
        "    print('Loading and preprocessing test data (bit = ' + str(bit) + ') ...')\n",
        "    print('-' * 30)\n",
        "\n",
        "    imgs_bit_test, imgs_mask_test, imgs_bit_id_test = load_test_data(bit)\n",
        "\n",
        "    imgs_bit_test = imgs_bit_test.astype('float32')\n",
        "    imgs_bit_test -= mean\n",
        "    imgs_bit_test /= std\n",
        "\n",
        "    print('-' * 30)\n",
        "    print('Loading saved weights...')\n",
        "    print('-' * 30)\n",
        "    model.load_weights('weights_resnet_' + str(bit) + '.h5')\n",
        "\n",
        "    print('-' * 30)\n",
        "    print('Predicting masks on test data (bit = ' + str(bit) + ') ...')\n",
        "    print('-' * 30)\n",
        "    imgs_mask_test = model.predict(imgs_bit_test, verbose=1)\n",
        "\n",
        "    if bit == 8:\n",
        "        print('-' * 30)\n",
        "        print('Saving predicted masks to files...')\n",
        "        print('-' * 30)\n",
        "        pred_dir = 'preds_8'\n",
        "        if not os.path.exists(pred_dir):\n",
        "            os.mkdir(pred_dir)\n",
        "        for image, image_id in zip(imgs_mask_test, imgs_bit_id_test):\n",
        "            image = (image[:, :, 0] * 255.).astype(np.uint8)\n",
        "            imsave(os.path.join(pred_dir, str(image_id).split('/')[-1] + '_pred_resnet.png'), image)\n",
        "\n",
        "    elif bit == 16:\n",
        "        print('-' * 30)\n",
        "        print('Saving predicted masks to files...')\n",
        "        print('-' * 30)\n",
        "        pred_dir = 'preds_16'\n",
        "        if not os.path.exists(pred_dir):\n",
        "            os.mkdir(pred_dir)\n",
        "        for image, image_id in zip(imgs_mask_test, imgs_bit_id_test):\n",
        "            image = (image[:, :, 0] * 255.).astype(np.uint8)\n",
        "            imsave(os.path.join(pred_dir, str(image_id).split('/')[-1] + '_pred_resnet.png'), image)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    train_and_predict(8)\n",
        "    #train_and_predict(16)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------------------\n",
            "Loading and train data (bit = 8) ...\n",
            "------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-76f52a4cd56c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m     \u001b[0mtrain_and_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m     \u001b[0;31m#train_and_predict(16)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-76f52a4cd56c>\u001b[0m in \u001b[0;36mtrain_and_predict\u001b[0;34m(bit)\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loading and train data (bit = '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbit\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m') ...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m     \u001b[0mimgs_bit_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimgs_bit_mask_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_train_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs_bit_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimgs_bit_mask_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-76f52a4cd56c>\u001b[0m in \u001b[0;36mload_train_data\u001b[0;34m(bit)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_train_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnpy_data_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'images_train.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mimages16\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnpy_data_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'images16_train.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mmasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnpy_data_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'masks_train.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_pathlib_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './npy/images_train.npy'"
          ]
        }
      ]
    }
  ]
}